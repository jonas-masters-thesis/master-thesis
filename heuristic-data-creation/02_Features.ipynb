{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53a7e6b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T11:18:13.753137Z",
     "iopub.status.busy": "2021-07-19T11:18:13.753137Z",
     "iopub.status.idle": "2021-07-19T11:18:13.763141Z",
     "shell.execute_reply": "2021-07-19T11:18:13.761148Z",
     "shell.execute_reply.started": "2021-07-19T11:18:13.753137Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from lexrank import LexRank\n",
    "from multiprocessing import Process, Pool\n",
    "from joblib import Parallel, delayed\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a7668aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-19T11:17:11.758753Z",
     "iopub.status.busy": "2021-07-19T11:17:11.758753Z",
     "iopub.status.idle": "2021-07-19T11:17:11.883760Z",
     "shell.execute_reply": "2021-07-19T11:17:11.882752Z",
     "shell.execute_reply.started": "2021-07-19T11:17:11.758753Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad443935",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T05:20:51.221106Z",
     "iopub.status.busy": "2021-07-05T05:20:51.220106Z",
     "iopub.status.idle": "2021-07-05T05:20:53.750095Z",
     "shell.execute_reply": "2021-07-05T05:20:53.749482Z",
     "shell.execute_reply.started": "2021-07-05T05:20:51.221106Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/filtered-w-reference-snippets-r_0.1-args-me.json', 'r') as f:\n",
    "    d = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1f212a",
   "metadata": {},
   "source": [
    "__Features__\n",
    "* Text features (as TS-ISF)\n",
    "    * unigrams, stemmed, stopwords removed\n",
    "    * bigrams, stemmed, stopwords removed\n",
    "    * entities, DBpedia spotlight\n",
    "* Surface features\n",
    "    * position, 1st, 2nd, 3rd, or later position\n",
    "    * number of words\n",
    "    * number of nouns (spacy)\n",
    "    * tfisf, sum of the TS-ISF scores for unigrams composing the sentence.\n",
    "    * btfisf, tfisf multiplied with 2(3) if a word appear in the first sentence\n",
    "    * LexRank scores, How?\n",
    "    \n",
    "__Additional__\n",
    "* Indicator whether the first token is a pronoun (snippet should be self-contained, [BarHaim.2020.b])\n",
    "* Argumentativeness\n",
    "* position, last, second-last, third-last, or earlier (arguments seem to have their sum-up at the end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f472f",
   "metadata": {},
   "source": [
    "__Collect all words__ [Maybe for later](https://github.com/dwyl/english-words/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e47b5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T05:20:54.986124Z",
     "iopub.status.busy": "2021-07-05T05:20:54.986124Z",
     "iopub.status.idle": "2021-07-05T05:23:55.679455Z",
     "shell.execute_reply": "2021-07-05T05:23:55.678630Z",
     "shell.execute_reply.started": "2021-07-05T05:20:54.986124Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 44279/44279 [03:00<00:00, 245.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocabulary = set()\n",
    "for argument in tqdm(d):\n",
    "    tokens = word_tokenize(argument['premises'][0]['text'])\n",
    "    vocabulary.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae6550f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T05:23:55.681492Z",
     "iopub.status.busy": "2021-07-05T05:23:55.680468Z",
     "iopub.status.idle": "2021-07-05T05:23:55.711508Z",
     "shell.execute_reply": "2021-07-05T05:23:55.710567Z",
     "shell.execute_reply.started": "2021-07-05T05:23:55.681492Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267809"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e884f91d",
   "metadata": {},
   "source": [
    "# Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d4f595",
   "metadata": {},
   "source": [
    "## Uni- and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64c1dc7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T05:23:55.714693Z",
     "iopub.status.busy": "2021-07-05T05:23:55.714330Z",
     "iopub.status.idle": "2021-07-05T05:23:55.744023Z",
     "shell.execute_reply": "2021-07-05T05:23:55.742467Z",
     "shell.execute_reply.started": "2021-07-05T05:23:55.714693Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(preprocessor=<function <lambda> at 0x0000025E13B1B8B8>,\n",
       "                stop_words='english',\n",
       "                tokenizer=<function word_tokenize at 0x0000025E70313AF8>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True, \n",
    "                             preprocessor = lambda s: re.sub('[^A-Za-z,.?!]', '', s),\n",
    "                             tokenizer=word_tokenize, \n",
    "                             stop_words='english', \n",
    "                             #ngram_range=(1,2), # Bigrams only for text features\n",
    "                             #vocabulary=vocabulary\n",
    "                            )\n",
    "\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "931b351e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T05:23:55.746270Z",
     "iopub.status.busy": "2021-07-05T05:23:55.745442Z",
     "iopub.status.idle": "2021-07-05T05:24:24.056594Z",
     "shell.execute_reply": "2021-07-05T05:24:24.048544Z",
     "shell.execute_reply.started": "2021-07-05T05:23:55.745442Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(preprocessor=<function <lambda> at 0x0000025E13B1B8B8>,\n",
       "                stop_words='english',\n",
       "                tokenizer=<function word_tokenize at 0x0000025E70313AF8>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc583edb",
   "metadata": {
    "tags": []
   },
   "source": [
    "%%time\n",
    "def trnsfrm(argument):\n",
    "    sentences = argument['premises'][0]['sentences']\n",
    "    ub_grams = vectorizer.transform(sentences)\n",
    "    argument['premises'][0]['ub_grams'] = ub_grams\n",
    "    \n",
    "Parallel(n_jobs=4, require='sharedmem')(delayed(trnsfrm)(arg) for arg in d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e2b0f11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T05:24:24.062536Z",
     "iopub.status.busy": "2021-07-05T05:24:24.061531Z",
     "iopub.status.idle": "2021-07-05T05:32:40.732260Z",
     "shell.execute_reply": "2021-07-05T05:32:40.731340Z",
     "shell.execute_reply.started": "2021-07-05T05:24:24.062536Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 44279/44279 [08:16<00:00, 89.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for argument in tqdm(d):\n",
    "    sentences = argument['premises'][0]['sentences']\n",
    "    ub_grams = vectorizer.transform(sentences)\n",
    "    argument['premises'][0]['ub_grams'] = ub_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a597bb3d",
   "metadata": {},
   "source": [
    "# Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b968dbfd",
   "metadata": {},
   "source": [
    "See [DBpedia API](https://www.dbpedia-spotlight.org/api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35dc4c95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-29T05:41:29.845944Z",
     "iopub.status.busy": "2021-06-29T05:41:29.844941Z",
     "iopub.status.idle": "2021-06-29T05:41:29.861915Z",
     "shell.execute_reply": "2021-06-29T05:41:29.860917Z",
     "shell.execute_reply.started": "2021-06-29T05:41:29.845944Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def spotting(argument):\n",
    "    sentences = argument['premises'][0]['sentences']\n",
    "    spotted = list()\n",
    "    for i, s in enumerate(sentences):\n",
    "        encoded = urllib.parse.quote(s)\n",
    "        response = requests.get(f'https://api.dbpedia-spotlight.org/en/spot?text={encoded}',  headers={\"accept\":\"application/json\"})\n",
    "        json_response = json.loads(response.text)\n",
    "        if 'surfaceForm' in json_response['annotation']:\n",
    "            spotted.append((i, json_response['annotation']['surfaceForm']['@name']))\n",
    "    argument['premises'][0]['entities'] = spotted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f5b49",
   "metadata": {},
   "source": [
    "## Surface Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f535a672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T05:36:22.170293Z",
     "iopub.status.busy": "2021-07-05T05:36:22.170293Z",
     "iopub.status.idle": "2021-07-05T05:36:22.182298Z",
     "shell.execute_reply": "2021-07-05T05:36:22.181293Z",
     "shell.execute_reply.started": "2021-07-05T05:36:22.170293Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def position(argument):\n",
    "    number_of_sents = len(argument['premises'][0]['sentences'])\n",
    "    values = [3 if i > 2 else i for i in range(number_of_sents)]\n",
    "    argument['premises'][0]['position'] = np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dc160f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T05:36:22.874458Z",
     "iopub.status.busy": "2021-07-05T05:36:22.873445Z",
     "iopub.status.idle": "2021-07-05T05:36:22.909447Z",
     "shell.execute_reply": "2021-07-05T05:36:22.908140Z",
     "shell.execute_reply.started": "2021-07-05T05:36:22.874458Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_words(argument):\n",
    "    counts = list()\n",
    "    for s in argument['premises'][0]['sentences']:\n",
    "        counts.append(len(word_tokenize(s)))\n",
    "    argument['premises'][0]['word_counts'] = np.array(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "941f6880",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T05:36:23.511088Z",
     "iopub.status.busy": "2021-07-05T05:36:23.510086Z",
     "iopub.status.idle": "2021-07-05T05:36:23.521084Z",
     "shell.execute_reply": "2021-07-05T05:36:23.519934Z",
     "shell.execute_reply.started": "2021-07-05T05:36:23.511088Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_nouns(argument):\n",
    "    counts = list()\n",
    "    for s in argument['premises'][0]['sentences']:\n",
    "        tags = pos_tag(word_tokenize(s))\n",
    "        count = sum([1 if 'NN' in t[1] else 0 for t in tags])\n",
    "        counts.append(count)\n",
    "    argument['premises'][0]['noun_counts'] = np.array(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0a9d8a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T05:36:24.181633Z",
     "iopub.status.busy": "2021-07-05T05:36:24.181633Z",
     "iopub.status.idle": "2021-07-05T05:36:24.196637Z",
     "shell.execute_reply": "2021-07-05T05:36:24.194637Z",
     "shell.execute_reply.started": "2021-07-05T05:36:24.181633Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tfisf(argument):\n",
    "    n = len(argument['premises'][0]['sentences'])\n",
    "    values = np.full(n, 0.0)\n",
    "    for i in range(n):\n",
    "        values[i] = np.sum(d[0]['premises'][0]['ub_grams'][:, i])\n",
    "        \n",
    "    argument['premises'][0]['tfisf'] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5698d71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T05:36:24.974343Z",
     "iopub.status.busy": "2021-07-05T05:36:24.973304Z",
     "iopub.status.idle": "2021-07-05T05:36:24.992306Z",
     "shell.execute_reply": "2021-07-05T05:36:24.991380Z",
     "shell.execute_reply.started": "2021-07-05T05:36:24.974343Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def btfisf(argument):\n",
    "    n = len(argument['premises'][0]['sentences'])\n",
    "    values = np.full(n, 0.0)\n",
    "    first_sent = d[0]['premises'][0]['ub_grams'][:, 0]\n",
    "    for i in range(n):\n",
    "        if i == 0:\n",
    "            values[i] = 3 * np.sum(d[0]['premises'][0]['ub_grams'][:, i])\n",
    "        else:\n",
    "            np.sum([w*3 if w in first_sent else w for w in d[0]['premises'][0]['ub_grams'][:, i]])\n",
    "        \n",
    "    argument['premises'][0]['btfisf'] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c3e0469",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T05:36:26.669789Z",
     "iopub.status.busy": "2021-07-05T05:36:26.668781Z",
     "iopub.status.idle": "2021-07-05T05:36:26.679816Z",
     "shell.execute_reply": "2021-07-05T05:36:26.678960Z",
     "shell.execute_reply.started": "2021-07-05T05:36:26.668781Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lr(argument):\n",
    "    sentences = argument['premises'][0]['sentences']\n",
    "    lxr = LexRank(sentences)\n",
    "    scores_cont = lxr.rank_sentences(\n",
    "        sentences,\n",
    "        threshold=None,\n",
    "        fast_power_method=False,\n",
    "    )\n",
    "    assert len(sentences) == len(scores_cont), f'Scores do not match sentences. sents = {len(sentences)}, scores = {scores_cont}'\n",
    "    argument['premises'][0]['lr'] = scores_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "199e6cad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T05:36:30.552678Z",
     "iopub.status.busy": "2021-07-05T05:36:30.552678Z",
     "iopub.status.idle": "2021-07-05T06:56:04.919381Z",
     "shell.execute_reply": "2021-07-05T06:56:04.918171Z",
     "shell.execute_reply.started": "2021-07-05T05:36:30.552678Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 44279/44279 [1:19:34<00:00,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 19min 34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for argument in tqdm(d):\n",
    "    position(argument)\n",
    "    count_words(argument)\n",
    "    count_nouns(argument)\n",
    "    tfisf(argument)\n",
    "    #btfisf(argument)\n",
    "    lr(argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aa348d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T06:56:04.920375Z",
     "iopub.status.busy": "2021-07-05T06:56:04.920375Z",
     "iopub.status.idle": "2021-07-05T06:56:04.934681Z",
     "shell.execute_reply": "2021-07-05T06:56:04.933409Z",
     "shell.execute_reply.started": "2021-07-05T06:56:04.920375Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3a729c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T07:41:31.517226Z",
     "iopub.status.busy": "2021-07-05T07:41:31.516230Z",
     "iopub.status.idle": "2021-07-05T07:41:35.262365Z",
     "shell.execute_reply": "2021-07-05T07:41:35.262365Z",
     "shell.execute_reply.started": "2021-07-05T07:41:31.517226Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/features.json', 'wb') as f:\n",
    "    pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8842e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-01T05:48:32.495242Z",
     "iopub.status.busy": "2021-07-01T05:48:32.495242Z",
     "iopub.status.idle": "2021-07-01T05:48:32.868200Z",
     "shell.execute_reply": "2021-07-01T05:48:32.867339Z",
     "shell.execute_reply.started": "2021-07-01T05:48:32.495242Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for argument in tqdm(d):\n",
    "    p0=Process(target=position, args=(argument,))\n",
    "    p0.start()\n",
    "    #p1=Process(target=count_words, args=(argument,))\n",
    "    #p1.start()\n",
    "    #p2=Process(target=count_nouns, args=(argument,))\n",
    "    #p2.start()\n",
    "    #p3=Process(target=tfisf, args=(argument,))\n",
    "    #p3.start()\n",
    "    #p4=Process(target=lr, args=(argument,))\n",
    "    #p4.start()\n",
    "    \n",
    "    p0.join()\n",
    "    #p1.join()\n",
    "    #p2.join()\n",
    "    #p3.join()\n",
    "    #p4.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c667e9",
   "metadata": {},
   "source": [
    "# Reversing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afe306a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why is it that so-called christians, Because there is no such a thing as a christian, Have serious trouble as READING and COMPREHENDING?',\n",
       " 'Its not that difficult, Nor is it that hard.',\n",
       " 'It was stated unto you a very simple \"* \"You are asking why God would forgive the murderer. \"',\n",
       " 'OK we\"re done.',\n",
       " 'You paid absolutely no attention whatsoever to the verses presented and instead went off into your own la la land. \"',\n",
       " 'But nah, All you did was babble on and on and on.',\n",
       " 'So in this sense, It was YOU that forfeited.',\n",
       " 'Sheesh!',\n",
       " 'Bye.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0]['premises'][0]['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21909dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7208751658653264"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(d[0]['premises'][0]['ub_grams'][:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dbbba81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = d[0]['premises'][0]['ub_grams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72907df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 26991)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f00cc919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['trouble', 'thing', 'so-called', 'reading', 'christians',\n",
       "        'christian', '?', ','], dtype='<U129'),\n",
       " array(['hard', 'difficult', '.', ','], dtype='<U129'),\n",
       " array(['unto', 'stated', 'simple', 'murderer.', 'god', 'forgive',\n",
       "        'asking', '``', '*'], dtype='<U129'),\n",
       " array(['ok', '.', \"''\"], dtype='<U129'),\n",
       " array(['whatsoever', 'went', 'verses', 'presented', 'paid', 'land.', 'la',\n",
       "        'instead', 'attention', 'absolutely', '``'], dtype='<U129'),\n",
       " array(['nah', 'did', 'babble', '.', ','], dtype='<U129'),\n",
       " array(['sense', 'forfeited', '.', ','], dtype='<U129'),\n",
       " array(['sheesh', '!'], dtype='<U129'),\n",
       " array(['bye', '.'], dtype='<U129')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_transform(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28a7aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = vec[0,:].nonzero()[1]\n",
    "scores = zip(fi, [vec[0,x] for x in fi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6fed223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trouble 0.320270212381216\n",
      "thing 0.2991483041507577\n",
      "so-called 0.320270212381216\n",
      "reading 0.2991483041507577\n",
      "christians 0.2991483041507577\n",
      "christian 0.3079146881227244\n",
      "? 0.320270212381216\n",
      ", 0.5735855597845322\n"
     ]
    }
   ],
   "source": [
    "for w, s in [(vectorizer.get_feature_names()[i], s) for (i, s) in scores]:\n",
    "    print(w,s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
